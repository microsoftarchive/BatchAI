{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python CNTK Distributed GPU Infiniband\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example uses the CIFAR-10 dataset to demonstrate how to train a Residual network (ResNet) on a multi-node multi-GPU cluster with infiniband. \n",
    "\n",
    "## Details\n",
    "\n",
    "- The official CNTK ResNet for CIFAR10 [example](https://github.com/Microsoft/CNTK/tree/master/Examples/Image/Classification/ResNet/Python) is used.\n",
    "- CIFAR-10 dataset(http://www.cs.toronto.edu/~kriz/cifar.html) has been preprocessed available at the [Azure storage](https://batchaisamples.blob.core.windows.net/samples/CIFAR-10_dataset.tar?st=2017-09-29T18%3A29%3A00Z&se=2099-12-31T08%3A00%3A00Z&sp=rl&sv=2016-05-31&sr=b&sig=nFXsAp0Eq%2BoS5%2BKAEPnfyEGlCkBcKIadDvCPA%2BcX6lU%3D), and will be downloaded to GPU local SSD. \n",
    "- The job will be run on a prebuild CNTK container ```batchaitraining/cntk:2.3-gpu-1bitsgd-py36-cuda8-cudnn6-intelmpi``` based on [dockerfile](./dockerfile). Intel MPI package will be installed in the container using job preparation command line.\n",
    "- For demonstration purposes, CIFAR-10 data preparation script and CNTK job scripts will be deployed at Azure File Share.\n",
    "- Standard output of the job and the model will be stored on Azure File Share.\n",
    "- This sample needs to use at lesat two STANDARD_NC24r nodes, please be sure you have enough quota\n",
    "- If you like to conduct performance comparasion with TCP network, you can create the cluster with VM size `STANDARD_NC24` that does not support Infiniband."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Install Dependencies and Create Configuration file.\n",
    "Follow [instructions](/recipes) to install all dependencies and create configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration and Create Batch AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "bfa11f00-8866-4051-bbfe-a9646e004910"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from azure.storage.file import FileService\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "# utilities.py contains helper functions used by different notebooks\n",
    "sys.path.append('../..')\n",
    "import utilities\n",
    "\n",
    "cfg = utilities.Configuration('../../configuration.json')\n",
    "client = utilities.create_batchai_client(cfg)\n",
    "utilities.create_resource_group(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Resoruce Group and Batch AI workspace if not exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utilities.create_resource_group(cfg)\n",
    "_ = client.workspaces.create(cfg.resource_group, cfg.workspace, cfg.location).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Compute Cluster\n",
    "\n",
    "- For this example we will use a gpu cluster of `STANDARD_NC24r` nodes, which equip with infiniband device. Number of nodes in the cluster is configured with `nodes_count` variable, and 2 nodes will be used by default.\n",
    "- Please be sure you have enough core quota to create at lesat two `STANDARD_NC24r` nodes.\n",
    "- We need to use the latest `UbuntuServer 16.04-LTS` as the host image, which is compatible with infiniband.\n",
    "- We will call the cluster `nc24r`\n",
    "- If you like to conduct performance comparasion with TCP network, you can create the cluster with VM size `STANDARD_NC24` that does not support Infiniband \n",
    "\n",
    "So, the cluster will have the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_count = 2\n",
    "cluster_name = 'nc24r'\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    location=cfg.location,\n",
    "    vm_size='STANDARD_NC24r',\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password or None,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key or None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cfg.workspace, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Cluster Creation\n",
    "\n",
    "Monitor the just created cluster. utilities.py contains a helper function to print out detail status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cfg.workspace, cluster_name)\n",
    "utilities.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create File Share\n",
    "\n",
    "For this example we will create a new File Share with name `batchaisample` under your storage account.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "azure_file_share_name = 'batchaisample'\n",
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_share(azure_file_share_name, fail_on_exist=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Sample Script and Configure the Input Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download original sample scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utilities.download_file('https://raw.githubusercontent.com/Microsoft/CNTK/v2.3/Examples/Image/Classification/ResNet/Python/resnet_models.py', 'resnet_models.py')\n",
    "utilities.download_file('https://raw.githubusercontent.com/Microsoft/CNTK/v2.3/Examples/Image/Classification/ResNet/Python/TrainResNet_CIFAR10_Distributed.py', 'TrainResNet_CIFAR10_Distributed.py')\n",
    "utilities.download_file('https://raw.githubusercontent.com/Microsoft/CNTK/v2.3/Examples/Image/Classification/ResNet/Python/TrainResNet_CIFAR10.py', 'TrainResNet_CIFAR10.py')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a folder on Azure File Share containing a copy of [TrainResNet_CIFAR10_Distributed.py](https://github.com/Microsoft/CNTK/blob/v2.3.1/Examples/Image/Classification/ResNet/Python/TrainResNet_CIFAR10_Distributed.py), [TrainResNet_CIFAR10.py](https://github.com/Microsoft/CNTK/blob/v2.3.1/Examples/Image/Classification/ResNet/Python/TrainResNet_CIFAR10.py) and [CIFAR-resnet_models.py](https://github.com/Microsoft/CNTK/blob/v2.3.1/Examples/Image/Classification/ResNet/Python/resnet_models.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cntk_script_path = 'cntk_samples'\n",
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.create_directory(\n",
    "    azure_file_share_name, cntk_script_path, fail_on_exist=False)\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, cntk_script_path, 'TrainResNet_CIFAR10_Distributed.py', 'TrainResNet_CIFAR10_Distributed.py')\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, cntk_script_path, 'TrainResNet_CIFAR10.py', 'TrainResNet_CIFAR10.py')\n",
    "service.create_file_from_path(\n",
    "    azure_file_share_name, cntk_script_path, 'resnet_models.py', 'resnet_models.py')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the [job preparation script](./jobprep_cntk_distributed_ib.sh), that does the following tasks:\n",
    "- Download CIFAR-10 data set on all GPU nodes (under ```$AZ_BATCHAI_JOB_TEMP``` directory)\n",
    "- Install IntelMPI binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service.create_file_from_path(\n",
    "    azure_file_share_name, cntk_script_path, 'jobprep_cntk_distributed_ib.sh', 'jobprep_cntk_distributed_ib.sh')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Job\n",
    "- The job will use `batchaitraining/cntk:2.3-gpu-1bitsgd-py36-cuda8-cudnn6-intelmpi` container that is built based on [dockerfile](./dockerfile)\n",
    "- Will use job preparation task to execute job prreparation script (jobprep_cntk_distributed_ib.sh). The CIFAR-10 dataset will be downloaded and processed on compute nodes locally (under ```$AZ_BATCHAI_JOB_TEMP``` directory);\n",
    "- Will run TrainResNet_CIFAR10_Distributed.py providing CIFAR-10 Dataset path as the first parameter and desired mode output as the second one. \n",
    "- Will set ```process_count``` to 8, so that all 8 GPUs from 2 NC24r nodes will be used;\n",
    "- For illustration purpose, we will train a ResNet 110 and only run 5 epoches\n",
    "- We will mount file share at folder with name `afs`. Full path of this folder on a computer node will be `$AZ_BATCHAI_MOUNT_ROOT/afs`.\n",
    "- The job needs to know where to find ConvNet_CIFAR10_DataAug_Distributed.py and input MNIST dataset. We will create two input directories for this\n",
    "    - Will be able to reference those directories using ```$AZ_BATCHAI_INPUT_SCRIPT``` environment variable.\n",
    "- The model output will be stored in File Share: will reference this directory as `$AZ_BATCHAI_OUTPUT_MODEL` and we will be able to enumerate files in this directory using `MODEL` id.\n",
    "- Will store standard and error output of the job in File Share too\n",
    "\n",
    "**Note** You must agree to the following licences before using this container:\n",
    "- [CNTK License](https://github.com/Microsoft/CNTK/blob/master/LICENSE.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "azure_file_share = 'afs'\n",
    "parameters = models.JobCreateParameters(\n",
    "     location=cfg.location,\n",
    "     cluster=models.ResourceId(id=cluster.id),\n",
    "     node_count=2,\n",
    "     input_directories=[\n",
    "        models.InputDirectory(\n",
    "            id='SCRIPT',\n",
    "            path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, cntk_script_path))],\n",
    "     std_out_err_path_prefix='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share),\n",
    "     output_directories=[\n",
    "        models.OutputDirectory(\n",
    "            id='MODEL',\n",
    "            path_prefix='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share))],\n",
    "     job_preparation=models.JobPreparation(command_line=\"bash $AZ_BATCHAI_INPUT_SCRIPT/jobprep_cntk_distributed_ib.sh\"),\n",
    "     container_settings=models.ContainerSettings(\n",
    "         image_source_registry=models.ImageSourceRegistry(\n",
    "             image='batchaitraining/cntk:2.3-gpu-1bitsgd-py36-cuda8-cudnn6-intelmpi')),\n",
    "     mount_volumes=models.MountVolumes(\n",
    "            azure_file_shares=[\n",
    "                models.AzureFileShareReference(\n",
    "                    account_name=cfg.storage_account_name,\n",
    "                    credentials=models.AzureStorageCredentialsInfo(\n",
    "                        account_key=cfg.storage_account_key),\n",
    "                    azure_file_url = 'https://{0}.file.core.windows.net/{1}'.format(\n",
    "                        cfg.storage_account_name, azure_file_share_name),\n",
    "                    relative_mount_path=azure_file_share)\n",
    "            ]\n",
    "        ),\n",
    "     cntk_settings = models.CNTKsettings(\n",
    "         python_script_file_path='$AZ_BATCHAI_INPUT_SCRIPT/TrainResNet_CIFAR10_Distributed.py',\n",
    "         command_line_args='--datadir $AZ_BATCHAI_JOB_TEMP --outputdir $AZ_BATCHAI_OUTPUT_MODEL -n resnet110 -e 105',       \n",
    "         process_count=8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training Job and wait for Job completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_name = 'cntk_experiment'\n",
    "experiment = client.experiments.create(cfg.resource_group, cfg.workspace, experiment_name).result()\n",
    "job_name = datetime.utcnow().strftime('cntk_%m_%d_%Y_%H%M%S')\n",
    "job = client.jobs.create(cfg.resource_group, cfg.workspace, experiment_name, job_name, parameters).result()\n",
    "print('Created Job {0} in Experiment {1}'.format(job.name, experiment.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Job to Finish\n",
    "The job will start running when the cluster will have enought idle nodes. The following code waits for job to start running printing the cluster state. During job run, the code prints current content of stderr.txt (the \n",
    "ConvNet_CIFAR10_DataAug_Distributed.py was changed to merge stdout and stderr output.)\n",
    "\n",
    "**Note** Execution may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "utilities.wait_for_job_completion(client, cfg.resource_group, cfg.workspace, \n",
    "                                  experiment_name, job_name, cluster_name, 'stdouterr', 'stdout.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### List stdout.txt and stderr.txt files for the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = client.jobs.list_output_files(cfg.resource_group, cfg.workspace, experiment_name, job_name,\n",
    "                                      models.JobsListOutputFilesOptions(outputdirectoryid='stdouterr')) \n",
    "for f in list(files):\n",
    "    print(f.name, f.download_url or 'directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.jobs.delete(cfg.resource_group, cfg.workspace, experiment_name, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = client.clusters.delete(cfg.resource_group, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.delete_share(azure_file_share_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
