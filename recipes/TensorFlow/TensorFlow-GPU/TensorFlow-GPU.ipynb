{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This example demonstrate how to run standard TensorFlow sample (https://github.com/tensorflow/models/blob/master/tutorials/image/mnist/convolutional.py) on Azure Batch AI cluster of one node.\n",
    "\n",
    "## Details\n",
    "\n",
    "- For demonstration purposes, official convolutional.py will be deployed at Azure File Share;\n",
    "- Standard output of the job will be stored on Azure File Share;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Install Dependencies and Create Configuration file.\n",
    "Follow [instructions](/recipes) to install all dependencies and create configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration and Create Batch AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "from azure.storage.file import FileService\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "# The BatchAI/utilities folder contains helper functions used by different notebooks\n",
    "sys.path.append('../../../..')\n",
    "import utilities as utils\n",
    "\n",
    "cfg = utils.config.Configuration('../../configuration.json')\n",
    "client = utils.config.create_batchai_client(cfg)\n",
    "utils.config.create_resource_group(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Resoruce Group and Batch AI workspace if not exists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.config.create_resource_group(cfg)\n",
    "_ = client.workspaces.create(cfg.resource_group, cfg.workspace, cfg.location).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Script in Azure Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure File Share\n",
    "\n",
    "For this example we will create a new File Share with name `batchaisample` under your storage account. This will be used to share the *training script file* and *output file*.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "azure_file_share_name = 'batchaisample'\n",
    "file_service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "file_service.create_share(azure_file_share_name, fail_on_exist=False)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Sample Script and Configure the Input Directories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download original sample script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/tensorflow/models/master/tutorials/image/mnist/convolutional.py ...Done\n"
     ]
    }
   ],
   "source": [
    "script_to_deploy = 'convolutional.py'\n",
    "utils.dataset.download_file('https://raw.githubusercontent.com/tensorflow/models/master/tutorials/image/mnist/convolutional.py', script_to_deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each job we will create a folder containing a copy of the sample script. This allows to run the same job with different scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "mnist_script_directory = 'tensorflow_samples'\n",
    "file_service.create_directory(\n",
    "    azure_file_share_name, mnist_script_directory, fail_on_exist=False)\n",
    "file_service.create_file_from_path(\n",
    "    azure_file_share_name, mnist_script_directory, script_to_deploy, script_to_deploy)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Azure Batch AI Compute Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Compute Cluster\n",
    "- For this example we will use a GPU cluster of 1 `STANDARD_NC6` node. You can increase the number of nodes by changing `nodes_count` variable;\n",
    "- We will call the cluster `nc6`;\n",
    "\n",
    "So, the cluster will have the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_count = 1\n",
    "cluster_name = 'nc6'\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    location=cfg.location,\n",
    "    vm_size='STANDARD_NC6',\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password or None,\n",
    "        admin_user_ssh_public_key=cfg.admin_ssh_key or None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.clusters.create(cfg.resource_group, cfg.workspace, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Cluster Creation\n",
    "\n",
    "Monitor the just created cluster. The `utilities` module contains a helper function to print out all kind of nodes count in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n"
     ]
    }
   ],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cfg.workspace, cluster_name)\n",
    "utils.cluster.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Azure Batch AI Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Job\n",
    "\n",
    "- The job will use `tensorflow/tensorflow:1.8.0-gpu` container.\n",
    "- Will use configured previously input and output directories.\n",
    "- Will mount file share at folder with name `afs`. Full path of this folder on a computer node will be `$AZ_BATCHAI_JOB_MOUNT_ROOT/afs`;\n",
    "- The job needs to know where to find ConvNet_MNIST.py. We will create an input directory, refered as ```AZ_BATCHAI_INPUT_SCRIPT``` for this.\n",
    "- We will store standard and error output of the job in File Share\n",
    "- By removing container_settings, the job will be ran on the host VMs if you are using DSVM.\n",
    "\n",
    "**Note** You must agree to the following licenses before using this container:\n",
    "- [TensorFlow License](https://github.com/tensorflow/tensorflow/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_file_share = 'afs'\n",
    "parameters = models.JobCreateParameters(\n",
    "     location=cfg.location,\n",
    "     cluster=models.ResourceId(id=cluster.id),\n",
    "     node_count=nodes_count,\n",
    "     input_directories=[\n",
    "        models.InputDirectory(\n",
    "            id='SCRIPT',\n",
    "            path='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, mnist_script_directory))],\n",
    "     std_out_err_path_prefix='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}'.format(azure_file_share),\n",
    "     mount_volumes=models.MountVolumes(\n",
    "            azure_file_shares=[\n",
    "                models.AzureFileShareReference(\n",
    "                    account_name=cfg.storage_account_name,\n",
    "                    credentials=models.AzureStorageCredentialsInfo(\n",
    "                        account_key=cfg.storage_account_key),\n",
    "                    azure_file_url='https://{0}.file.core.windows.net/{1}'.format(\n",
    "                        cfg.storage_account_name, azure_file_share_name),\n",
    "                    relative_mount_path=azure_file_share)\n",
    "            ]\n",
    "        ),\n",
    "     environment_variables=[models.EnvironmentVariable(\n",
    "                name='MY_OUTPUT_SCRIPT',\n",
    "                value='$AZ_BATCHAI_JOB_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, mnist_script_directory))],\n",
    "     job_preparation=models.JobPreparation(command_line='cd $AZ_BATCHAI_JOB_TEMP; echo \"import os,sys,time\" > wrapper.py;echo \"print(sys.argv[1])\" >> wrapper.py;echo \"os.system(sys.argv[1])\" >> wrapper.py'),\n",
    "\n",
    "     container_settings=models.ContainerSettings(\n",
    "         image_source_registry=models.ImageSourceRegistry(image='tensorflow/tensorflow:1.8.0-gpu')),\n",
    "     tensor_flow_settings=models.TensorFlowSettings(\n",
    "         python_script_file_path='$AZ_BATCHAI_JOB_TEMP/wrapper.py \"python $MY_OUTPUT_SCRIPT/convolutional.py\"'\n",
    "     )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training Job and wait for Job completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Job tf_07_05_2018_215237 in Experiment tensorflow_experiment\n"
     ]
    }
   ],
   "source": [
    "experiment_name = 'tensorflow_experiment'\n",
    "experiment = client.experiments.create(cfg.resource_group, cfg.workspace, experiment_name).result()\n",
    "job_name = datetime.utcnow().strftime('tf_%m_%d_%Y_%H%M%S')\n",
    "job = client.jobs.create(cfg.resource_group, cfg.workspace, experiment_name, job_name, parameters).result()\n",
    "print('Created Job {0} in Experiment {1}'.format(job.name, experiment.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Job to Finish\n",
    "The job will start running when the cluster will have enough idle nodes. The following code waits for job to start running printing the cluster state. During job run, the code prints current content of stdeout-0.txt (the output of the worker running on the first node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: resizing Target: 1; Allocated: 0; Idle: 0; Unusable: 0; Running: 0; Preparing: 0; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 0; Preparing: 1; Leaving: 0\n",
      "Job state: queued ExitCode: None\n",
      "Cluster state: steady Target: 1; Allocated: 1; Idle: 0; Unusable: 0; Running: 1; Preparing: 0; Leaving: 0\n",
      "Job state: running ExitCode: None\n",
      "Waiting for job output to become available...\n",
      "python /mnt/batch/tasks/shared/LS_root/jobs/kevin_workspace/tensorflow_experiment/tf_07_05_2018_215237/mounts/afs/tensorflow_samples/convolutional.py\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Initialized!\n",
      "Step 0 (epoch 0.00), 12.5 ms\n",
      "Minibatch loss: 8.334, learning rate: 0.010000\n",
      "Minibatch error: 85.9%\n",
      "Validation error: 84.6%\n",
      "Step 100 (epoch 0.12), 9.5 ms\n",
      "Minibatch loss: 3.270, learning rate: 0.010000\n",
      "Minibatch error: 9.4%\n",
      "Validation error: 7.7%\n",
      "Step 200 (epoch 0.23), 8.1 ms\n",
      "Minibatch loss: 3.355, learning rate: 0.010000\n",
      "Minibatch error: 10.9%\n",
      "Validation error: 4.4%\n",
      "Step 300 (epoch 0.35), 8.1 ms\n",
      "Minibatch loss: 3.158, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 3.0%\n",
      "Step 400 (epoch 0.47), 8.1 ms\n",
      "Minibatch loss: 3.233, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 2.7%\n",
      "Step 500 (epoch 0.58), 8.1 ms\n",
      "Minibatch loss: 3.166, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 2.7%\n",
      "Step 600 (epoch 0.70), 8.1 ms\n",
      "Minibatch loss: 3.147, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.3%\n",
      "Step 700 (epoch 0.81), 8.1 ms\n",
      "Minibatch loss: 2.979, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.1%\n",
      "Step 800 (epoch 0.93), 8.2 ms\n",
      "Minibatch loss: 3.081, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 2.0%\n",
      "Step 900 (epoch 1.05), 8.1 ms\n",
      "Minibatch loss: 2.902, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.7%\n",
      "Step 1000 (epoch 1.16), 8.1 ms\n",
      "Minibatch loss: 2.899, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.0%\n",
      "Step 1100 (epoch 1.28), 8.1 ms\n",
      "Minibatch loss: 2.820, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 1200 (epoch 1.40), 8.1 ms\n",
      "Minibatch loss: 2.924, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.7%\n",
      "Step 1300 (epoch 1.51), 8.1 ms\n",
      "Minibatch loss: 2.793, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.7%\n",
      "Step 1400 (epoch 1.63), 8.1 ms\n",
      "Minibatch loss: 2.832, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.6%\n",
      "Step 1500 (epoch 1.75), 8.1 ms\n",
      "Minibatch loss: 2.825, learning rate: 0.009500\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.3%\n",
      "Step 1600 (epoch 1.86), 8.1 ms\n",
      "Minibatch loss: 2.729, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 1700 (epoch 1.98), 8.2 ms\n",
      "Minibatch loss: 2.657, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 1800 (epoch 2.09), 8.1 ms\n",
      "Minibatch loss: 2.669, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 1900 (epoch 2.21), 8.1 ms\n",
      "Minibatch loss: 2.638, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 2000 (epoch 2.33), 8.1 ms\n",
      "Minibatch loss: 2.608, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2100 (epoch 2.44), 8.1 ms\n",
      "Minibatch loss: 2.566, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2200 (epoch 2.56), 8.1 ms\n",
      "Minibatch loss: 2.566, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2300 (epoch 2.68), 8.1 ms\n",
      "Minibatch loss: 2.596, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 2400 (epoch 2.79), 8.1 ms\n",
      "Minibatch loss: 2.524, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 2500 (epoch 2.91), 8.1 ms\n",
      "Minibatch loss: 2.471, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 2600 (epoch 3.03), 8.2 ms\n",
      "Minibatch loss: 2.480, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2700 (epoch 3.14), 8.1 ms\n",
      "Minibatch loss: 2.517, learning rate: 0.008574\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 2800 (epoch 3.26), 8.1 ms\n",
      "Minibatch loss: 2.451, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2900 (epoch 3.37), 8.2 ms\n",
      "Minibatch loss: 2.509, learning rate: 0.008574\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 3000 (epoch 3.49), 8.1 ms\n",
      "Minibatch loss: 2.383, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3100 (epoch 3.61), 8.1 ms\n",
      "Minibatch loss: 2.388, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 3200 (epoch 3.72), 8.1 ms\n",
      "Minibatch loss: 2.353, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 3300 (epoch 3.84), 8.1 ms\n",
      "Minibatch loss: 2.337, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 3400 (epoch 3.96), 8.1 ms\n",
      "Minibatch loss: 2.288, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 3500 (epoch 4.07), 8.2 ms\n",
      "Minibatch loss: 2.281, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3600 (epoch 4.19), 8.3 ms\n",
      "Minibatch loss: 2.257, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3700 (epoch 4.31), 8.1 ms\n",
      "Minibatch loss: 2.229, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3800 (epoch 4.42), 8.1 ms\n",
      "Minibatch loss: 2.215, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3900 (epoch 4.54), 8.3 ms\n",
      "Minibatch loss: 2.231, learning rate: 0.008145\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 4000 (epoch 4.65), 8.1 ms\n",
      "Minibatch loss: 2.254, learning rate: 0.008145\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 4100 (epoch 4.77), 8.2 ms\n",
      "Minibatch loss: 2.171, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Step 4200 (epoch 4.89), 8.1 ms\n",
      "Minibatch loss: 2.167, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 4300 (epoch 5.00), 8.1 ms\n",
      "Minibatch loss: 2.187, learning rate: 0.007738\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 4400 (epoch 5.12), 8.2 ms\n",
      "Minibatch loss: 2.125, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4500 (epoch 5.24), 8.2 ms\n",
      "Minibatch loss: 2.164, learning rate: 0.007738\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.9%\n",
      "Step 4600 (epoch 5.35), 8.1 ms\n",
      "Minibatch loss: 2.081, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 4700 (epoch 5.47), 8.2 ms\n",
      "Minibatch loss: 2.074, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 4800 (epoch 5.59), 8.1 ms\n",
      "Minibatch loss: 2.052, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4900 (epoch 5.70), 8.1 ms\n",
      "Minibatch loss: 2.054, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 5000 (epoch 5.82), 8.1 ms\n",
      "Minibatch loss: 2.073, learning rate: 0.007738\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.9%\n",
      "Step 5100 (epoch 5.93), 8.1 ms\n",
      "Minibatch loss: 1.999, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 5200 (epoch 6.05), 8.1 ms\n",
      "Minibatch loss: 2.125, learning rate: 0.007351\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.0%\n",
      "Step 5300 (epoch 6.17), 8.1 ms\n",
      "Minibatch loss: 1.979, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 5400 (epoch 6.28), 8.2 ms\n",
      "Minibatch loss: 1.959, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 5500 (epoch 6.40), 8.2 ms\n",
      "Minibatch loss: 2.017, learning rate: 0.007351\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 5600 (epoch 6.52), 8.1 ms\n",
      "Minibatch loss: 1.929, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5700 (epoch 6.63), 8.1 ms\n",
      "Minibatch loss: 1.913, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5800 (epoch 6.75), 8.1 ms\n",
      "Minibatch loss: 1.899, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5900 (epoch 6.87), 8.1 ms\n",
      "Minibatch loss: 1.891, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 6000 (epoch 6.98), 8.1 ms\n",
      "Minibatch loss: 1.902, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 6100 (epoch 7.10), 8.1 ms\n",
      "Minibatch loss: 1.875, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6200 (epoch 7.21), 8.2 ms\n",
      "Minibatch loss: 1.845, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6300 (epoch 7.33), 8.1 ms\n",
      "Minibatch loss: 1.836, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 6400 (epoch 7.45), 8.1 ms\n",
      "Minibatch loss: 1.827, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6500 (epoch 7.56), 8.1 ms\n",
      "Minibatch loss: 1.806, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6600 (epoch 7.68), 8.2 ms\n",
      "Minibatch loss: 1.841, learning rate: 0.006983\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 6700 (epoch 7.80), 8.3 ms\n",
      "Minibatch loss: 1.782, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6800 (epoch 7.91), 8.2 ms\n",
      "Minibatch loss: 1.769, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6900 (epoch 8.03), 8.1 ms\n",
      "Minibatch loss: 1.757, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 7000 (epoch 8.15), 8.1 ms\n",
      "Minibatch loss: 1.749, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 7100 (epoch 8.26), 8.1 ms\n",
      "Minibatch loss: 1.734, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7200 (epoch 8.38), 8.1 ms\n",
      "Minibatch loss: 1.736, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 7300 (epoch 8.49), 8.2 ms\n",
      "Minibatch loss: 1.767, learning rate: 0.006634\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.8%\n",
      "Step 7400 (epoch 8.61), 8.1 ms\n",
      "Minibatch loss: 1.701, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7500 (epoch 8.73), 8.1 ms\n",
      "Minibatch loss: 1.694, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7600 (epoch 8.84), 8.1 ms\n",
      "Minibatch loss: 1.728, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Step 7700 (epoch 8.96), 8.1 ms\n",
      "Minibatch loss: 1.666, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 7800 (epoch 9.08), 8.2 ms\n",
      "Minibatch loss: 1.661, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7900 (epoch 9.19), 8.1 ms\n",
      "Minibatch loss: 1.646, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8000 (epoch 9.31), 8.2 ms\n",
      "Minibatch loss: 1.652, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 8100 (epoch 9.43), 8.4 ms\n",
      "Minibatch loss: 1.631, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8200 (epoch 9.54), 8.2 ms\n",
      "Minibatch loss: 1.623, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 8300 (epoch 9.66), 8.2 ms\n",
      "Minibatch loss: 1.618, learning rate: 0.006302\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 8400 (epoch 9.77), 8.2 ms\n",
      "Minibatch loss: 1.596, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8500 (epoch 9.89), 8.1 ms\n",
      "Minibatch loss: 1.593, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Test error: 0.8%\n",
      "Job state: succeeded ExitCode: 0\n"
     ]
    }
   ],
   "source": [
    "utils.job.wait_for_job_completion(client, cfg.resource_group, cfg.workspace, \n",
    "                                  experiment_name, job_name, cluster_name, 'stdouterr', 'stdout-wk-0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### List stdout.txt and stderr.txt files for the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "execution.log https://teststoragewewa.file.core.windows.net/batchaisample/1cba1da6-5a83-45e1-a88e-8b397eb84356/t-wewa/workspaces/kevin_workspace/experiments/tensorflow_experiment/jobs/tf_07_05_2018_215237/81ecd575-c92e-419c-8738-a5b32157230c/stdouterr/execution.log?sv=2016-05-31&sr=f&sig=QRmftGTJCBc8RDxwa5Fk8YZ8bPGOwHSgw2Hs%2FSIx%2FEs%3D&se=2018-07-05T23%3A11%3A49Z&sp=rl\n",
      "stderr-job_prep.txt https://teststoragewewa.file.core.windows.net/batchaisample/1cba1da6-5a83-45e1-a88e-8b397eb84356/t-wewa/workspaces/kevin_workspace/experiments/tensorflow_experiment/jobs/tf_07_05_2018_215237/81ecd575-c92e-419c-8738-a5b32157230c/stdouterr/stderr-job_prep.txt?sv=2016-05-31&sr=f&sig=YIJJmchMc%2F%2FDJujTQdkqfbkly8CPeagIR%2FVVTUYyMLQ%3D&se=2018-07-05T23%3A11%3A49Z&sp=rl\n",
      "stderr-wk-0.txt https://teststoragewewa.file.core.windows.net/batchaisample/1cba1da6-5a83-45e1-a88e-8b397eb84356/t-wewa/workspaces/kevin_workspace/experiments/tensorflow_experiment/jobs/tf_07_05_2018_215237/81ecd575-c92e-419c-8738-a5b32157230c/stdouterr/stderr-wk-0.txt?sv=2016-05-31&sr=f&sig=MKv%2Bsr9OwTqkQV%2Fn4XnI6VytFA3pg%2FMWGv4TDG80%2BWE%3D&se=2018-07-05T23%3A11%3A49Z&sp=rl\n",
      "stdout-job_prep.txt https://teststoragewewa.file.core.windows.net/batchaisample/1cba1da6-5a83-45e1-a88e-8b397eb84356/t-wewa/workspaces/kevin_workspace/experiments/tensorflow_experiment/jobs/tf_07_05_2018_215237/81ecd575-c92e-419c-8738-a5b32157230c/stdouterr/stdout-job_prep.txt?sv=2016-05-31&sr=f&sig=gxpw43QV4gbfTyWPMTuu9v6s8FxKCiMOxS%2FUive90TQ%3D&se=2018-07-05T23%3A11%3A49Z&sp=rl\n",
      "stdout-wk-0.txt https://teststoragewewa.file.core.windows.net/batchaisample/1cba1da6-5a83-45e1-a88e-8b397eb84356/t-wewa/workspaces/kevin_workspace/experiments/tensorflow_experiment/jobs/tf_07_05_2018_215237/81ecd575-c92e-419c-8738-a5b32157230c/stdouterr/stdout-wk-0.txt?sv=2016-05-31&sr=f&sig=wCyKtQcAX%2F%2F6L%2BcYte6PpY5nGz5%2B8L%2FgM4KJ6z0eHpA%3D&se=2018-07-05T23%3A11%3A49Z&sp=rl\n"
     ]
    }
   ],
   "source": [
    "files = client.jobs.list_output_files(cfg.resource_group, cfg.workspace, experiment_name, job_name,\n",
    "                                      models.JobsListOutputFilesOptions(outputdirectoryid='stdouterr')) \n",
    "for f in list(files):\n",
    "    print(f.name, f.download_url or 'directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.jobs.delete(cfg.resource_group, cfg.workspace, experiment_name, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = client.clusters.delete(cfg.resource_group, cfg.workspace, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = BlobService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.delete_container('batchaisample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
