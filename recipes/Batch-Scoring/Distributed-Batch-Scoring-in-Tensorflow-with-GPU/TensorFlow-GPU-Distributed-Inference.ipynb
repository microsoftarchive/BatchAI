{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Batch Scoring in Tensorflow with GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrate how to run distributed batch scoring job in TensorFlow on Azure Batch AI cluster of 2 nodes. [Inception-V3](https://arxiv.org/abs/1512.00567) model and unlabeled images from [ImageNet](http://image-net.org/) dataset will be used.\n",
    "\n",
    "## Details\n",
    "\n",
    "- For demonstration purposes, pretained [Inception-V3](https://arxiv.org/abs/1512.00567) model and approxinately 900 evaluation images from [ImageNet](http://image-net.org/) dataset will be deployed to Azure Blob Container\n",
    "- Standard output of the job will be stored on Azure File Share;\n",
    "- Azure Blob Container and Azure File Share will be mounted on Batch AI GPU clusters \n",
    "- The recipe uses [batch_image_label.py](./batch_image_label.py) script to perform Distributed Batch Scoring with the given model and image datasets. The input images for evaluation will be partitioned by the MPI rank, so that each MPI worker will evaluate part of the whole image set independently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "### Install Dependencies and Create Configuration file.\n",
    "Follow [instructions](/recipes) to install all dependencies and create configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Configuration and Create Batch AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import zipfile, tarfile\n",
    "\n",
    "from azure.storage.file import FileService\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import azure.mgmt.batchai.models as models\n",
    "\n",
    "# utilities.py contains helper functions used by different notebooks\n",
    "sys.path.append('../..')\n",
    "import utilities\n",
    "\n",
    "cfg = utilities.Configuration('../../configuration.json')\n",
    "client = utilities.create_batchai_client(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare Training Dataset and Script in Azure Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure Blob Container\n",
    "\n",
    "We will create a new Blob Container with name `batchaisample` under your storage account. This will be used to store the *input training dataset*\n",
    "\n",
    "**Note** You don't need to create new blob Container for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "azure_blob_container_name = 'batchaisample'\n",
    "blob_service = BlockBlobService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "blob_service.create_container(azure_blob_container_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Upload MNIST Dataset to Azure Blob Container\n",
    "\n",
    "For demonstration purposes, we will download pretrained `Inception-V3` model and a set of imagenet evaluation images to the current directory and upload it to Azure Blob Container.\n",
    "\n",
    "The following code downloads the above resource to the current local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_url = 'http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz'\n",
    "utilities.download_file(model_url, 'inception_v3.tar.gz')\n",
    "with  tarfile.open('inception_v3.tar.gz', \"r:gz\") as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "images_url = 'https://batchaisamples.blob.core.windows.net/samples/imagenet_samples.zip?st=2017-09-29T18%3A29%3A00Z&se=2099-12-31T08%3A00%3A00Z&sp=rl&sv=2016-05-31&sr=c&sig=PmhL%2BYnYAyNTZr1DM2JySvrI12e%2F4wZNIwCtf7TRI%2BM%3D'\n",
    "utilities.download_file(images_url, 'imagenet_samples.zip')\n",
    "with zipfile.ZipFile('imagenet_samples.zip', 'r') as z:\n",
    "    z.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to create folders and upload files into Azure Blob Container - you can use [Azure Portal](https://ms.portal.azure.com), [Storage Explorer](http://storageexplorer.com/), [Azure CLI2](/azure-cli-extension) or Azure SDK for your preferable programming language.\n",
    "In this example we will use Azure SDK for python to copy files into Blob."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the pretained model and output labels to Azure Blob Container directory named `pretained_models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Uploading pretained model and output labels...')\n",
    "model_directory = 'pretained_models'\n",
    "blob_service.create_blob_from_path(azure_blob_container_name, \n",
    "                                   model_directory + '/' + 'inception_v3.ckpt', 'inception_v3.ckpt')\n",
    "blob_service.create_blob_from_path(azure_blob_container_name, \n",
    "                                   model_directory + '/' + 'imagenet_slim_labels.txt', 'imagenet_slim_labels.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload imagenet image samples to Azure Blob Container directory named `unlabeled_images`. This step may take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Uploading sample images to evaluate...')\n",
    "image_directory = 'unlabeled_images'\n",
    "for f in os.listdir('samples'): \n",
    "    if os.path.isfile(os.path.join('samples', f)):\n",
    "        blob_service.create_blob_from_path(azure_blob_container_name,\n",
    "                                               image_directory + '/' + f, os.path.join('samples', f))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure File Share\n",
    "\n",
    "For this example we will create a new File Share with name `batchaisample` under your storage account. This will be used to share the *training script file* and *output file*.\n",
    "\n",
    "**Note** You don't need to create new file share for every cluster. We are doing this in this sample to simplify resource management for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "azure_file_share_name = 'batchaisample'\n",
    "file_service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "file_service.create_share(azure_file_share_name, fail_on_exist=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Sample Script to Azure File Share\n",
    "For each job we will create a folder containing a copy of the sample script. This allows to run the same job with different scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "script_directory = 'classification_samples'\n",
    "script_to_deploy = 'batch_image_label.py'\n",
    "file_service.create_directory(\n",
    "    azure_file_share_name, script_directory, fail_on_exist=False)\n",
    "file_service.create_file_from_path(\n",
    "    azure_file_share_name, script_directory, script_to_deploy, script_to_deploy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Azure Batch AI Compute Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Compute Cluster\n",
    "- For this example we will use a gpu cluster of 2 `STANDARD_NC6` nodes. You can increase the number of nodes by changing `nodes_count` variable\n",
    "- We will mount file share at folder with name `afs`. Full path of this folder on a computer node will be `$AZ_BATCHAI_MOUNT_ROOT/afs`\n",
    "- We will mount Azure Blob Container at folder with name `bfs`. Full path of this folder on a computer node will be `$AZ_BATCHAI_MOUNT_ROOT/bfs`\n",
    "- We will call the cluster `nc6`\n",
    "\n",
    "So, the cluster will have the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "azure_file_share = 'afs'\n",
    "azure_blob = 'bfs'\n",
    "\n",
    "nodes_count = 2\n",
    "cluster_name = 'nc62'\n",
    "\n",
    "volumes = models.MountVolumes(\n",
    "    azure_file_shares=[\n",
    "        models.AzureFileShareReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            azure_file_url='https://{0}.file.core.windows.net/{1}'.format(\n",
    "                cfg.storage_account_name, azure_file_share_name),\n",
    "            relative_mount_path=azure_file_share)\n",
    "    ],\n",
    "    azure_blob_file_systems=[\n",
    "        models.AzureBlobFileSystemReference(\n",
    "            account_name=cfg.storage_account_name,\n",
    "            credentials=models.AzureStorageCredentialsInfo(\n",
    "                account_key=cfg.storage_account_key),\n",
    "            container_name=azure_blob_container_name,\n",
    "            relative_mount_path=azure_blob)\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = models.ClusterCreateParameters(\n",
    "    location=cfg.location,\n",
    "    vm_size=\"STANDARD_NC6\",\n",
    "    scale_settings=models.ScaleSettings(\n",
    "        manual=models.ManualScaleSettings(target_node_count=nodes_count)\n",
    "    ),\n",
    "    node_setup=models.NodeSetup(\n",
    "        mount_volumes=volumes,\n",
    "    ),\n",
    "    user_account_settings=models.UserAccountSettings(\n",
    "        admin_user_name=cfg.admin,\n",
    "        admin_user_password=cfg.admin_password,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster = client.clusters.create(cfg.resource_group, cluster_name, parameters).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Cluster Creation\n",
    "\n",
    "Monitor the just created cluster. utilities.py contains a helper function to print out detail status of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster = client.clusters.get(cfg.resource_group, cluster_name)\n",
    "utilities.print_cluster_status(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Azure Batch AI Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Job\n",
    "\n",
    "- The job will use `tensorflow/tensorflow:1.7.0-gpu` container.\n",
    "- Will install job preparation task to install OpenMPI binary.\n",
    "- Will use custom toolkit to launch MPI processes.\n",
    "- In [`batch_image_label.py`](./batch_image_label.py), the input images for evaluation will be partitioned by the MPI rank, so that each MPI worker will evaluate part of the whole image set independently. \n",
    "\n",
    "**Note** You must agree to the following licences before using this container:\n",
    "- [TensorFlow License](https://github.com/tensorflow/tensorflow/blob/master/LICENSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mpi_launcher = 'mpirun -mca btl_tcp_if_exclude docker0,lo --allow-run-as-root --hostfile $AZ_BATCHAI_MPI_HOST_FILE '\n",
    "parameters = models.JobCreateParameters(\n",
    "     location=cfg.location,\n",
    "     cluster=models.ResourceId(id=cluster.id),\n",
    "     node_count=2,\n",
    "     input_directories=[\n",
    "        models.InputDirectory(\n",
    "            id='SCRIPT',\n",
    "            path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_file_share, script_directory)),\n",
    "        models.InputDirectory(\n",
    "            id='IMAGES',\n",
    "            path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_blob, image_directory)),\n",
    "        models.InputDirectory(\n",
    "            id='MODEL',\n",
    "            path='$AZ_BATCHAI_MOUNT_ROOT/{0}/{1}'.format(azure_blob, model_directory))],\n",
    "     std_out_err_path_prefix=\"$AZ_BATCHAI_MOUNT_ROOT/{0}\".format(azure_file_share),\n",
    "     output_directories=[\n",
    "        models.OutputDirectory(\n",
    "            id='LABEL',\n",
    "            path_prefix='$AZ_BATCHAI_MOUNT_ROOT/{0}'.format(azure_file_share))],\n",
    "     container_settings=models.ContainerSettings(\n",
    "         image_source_registry=models.ImageSourceRegistry(image='tensorflow/tensorflow:1.7.0-gpu')),\n",
    "     job_preparation=models.JobPreparation(\n",
    "         command_line=\"apt update && apt install -y mpi-default-dev mpi-default-bin\"),\n",
    "     custom_toolkit_settings = models.CustomToolkitSettings(\n",
    "         command_line=mpi_launcher+'python -u $AZ_BATCHAI_INPUT_SCRIPT/batch_image_label.py --dataset_path $AZ_BATCHAI_INPUT_IMAGES --model_path $AZ_BATCHAI_INPUT_MODEL/inception_v3.ckpt --label_path $AZ_BATCHAI_INPUT_MODEL/imagenet_slim_labels.txt --output_dir $AZ_BATCHAI_OUTPUT_LABEL --batch_size 64'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training Job and wait for Job completion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job_name = datetime.utcnow().strftime(\"classification_%m_%d_%Y_%H%M%S\")\n",
    "job = client.jobs.create(cfg.resource_group, job_name, parameters).result()\n",
    "print('Created Job: {}'.format(job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Job to Finish\n",
    "The job will start running when the cluster will have enought idle nodes. The following code waits for job to start running printing the cluster state. During job run, the code prints current content of stdeout-0.txt (the output of the worker running on the first node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "utilities.wait_for_job_completion(client, cfg.resource_group, job_name, cluster_name, 'stdouterr', 'stdout.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Download Image Output Label file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = client.jobs.list_output_files(cfg.resource_group, job_name, models.JobsListOutputFilesOptions(outputdirectoryid=\"LABEL\")) \n",
    "for f in list(files):\n",
    "    print(f.name + \": \" + str(f.content_length) + ' Bytes')\n",
    "    utilities.download_file(f.download_url, f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the downloaded classification output files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n in range(nodes_count):\n",
    "    print('result-labels-{0}.txt content:'.format(n))\n",
    "    with open('result-labels-{0}.txt'.format(n)) as f:\n",
    "        print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean Up (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = client.jobs.delete(cfg.resource_group, job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the Cluster\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the cluster using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = client.clusters.delete(cfg.resource_group, cluster_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Delete File Share\n",
    "When you are finished with the sample and don't want to submit any more jobs you can delete the file share completely with all files using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service = FileService(cfg.storage_account_name, cfg.storage_account_key)\n",
    "service.delete_share(azure_file_share_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
